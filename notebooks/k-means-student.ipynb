{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\"><center>K-MEANS CLUSTERING USING APACHE SPARK</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several machine learning clustering examples available online using various implementations such as scikit-learn or other packages. Below is one such example describing how to use a k-means algorithm on randomly generated two dimensional data with the Apache Spark implementation. \n",
    "\n",
    "### After covering the first basic example with generic data, we will subsequently look at a second example on more realistic customer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\"><center>PART 1: GENERIC EXAMPLE</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 1: Import and declare a few variables which will be used in the subsequent cells**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print (pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 2: Create the generic data set using the pandas make_blobs method**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input parameters in the cell below have the following meanings:\n",
    "1- num_samples: Total numbers of datapoints which will be generated.<br>\n",
    "2- num_features: Number of dimensions associated with each datapoint. For example, num_features = 2 means that each point has two coordinates x and y\n",
    "\n",
    "The **make_blobs** method will take one more parameter indicating the number of clusters which we would like to use. The total number of datapoints to be generated will be distributed around the center point of those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_samples=10000\n",
    "#n_features=3\n",
    "num_samples = 3000 #Total number of points\n",
    "num_features = 2 #2D datapoints\n",
    "num_clusters = 6\n",
    "X, y = make_blobs(n_samples=num_samples, centers=num_clusters, n_features=num_features, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The make_blobs method returns two arrays, which we will label X and y:\n",
    "1- X is the array of all num_sample datapoints which were generated.<br>\n",
    "2- y is a corresponding array providing the cluster which the datapoint at the same offset in x belongs to. For example, the first entry in x is a datapoint coordinates and the corresponding first entry in y will be the cluster that this datapoint belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can take a quick look at those two arrays X and y to get a feel for their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next few steps, we are going to convert those two arrays X and y to a dataframe which will allow us to later bring the data into a machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Createa a dataframe named df with two columns 'x' and 'y' which will contain the coordinates of the points from the original array.<br>\n",
    "2- Add to the dataframe a third column named 'id'. This column will have the keyword 'row' augmented with the index of the current row, thereby uniquely identify each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a row index as a string\n",
    "dfpandas = pd.DataFrame(X, columns=['x', 'y'])\n",
    "dfpandas['id'] = 'row'+dfpandas.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now move the id column to the front (left) of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dfpandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dfpandas)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "dfpandas = dfpandas.ix[:, cols]\n",
    "dfpandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If desired, we can save the data as a local csv file and reload it later. In the current example, we will directly convert the pandas dataframe to a Spark one.\n",
    "# save the ndarray as a csv file\n",
    "#df.to_csv('input.csv', index=False)\n",
    "#!cat input.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = plt.figure(figsize=(12,10)).gca(projection='rectilinear')\n",
    "myplot.scatter(X[:,0], X[:,1], c=y)\n",
    "myplot.set_xlabel('x')\n",
    "myplot.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL = ['x', 'y']\n",
    "path = 'input.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 3: Create a Spark dataframe and cast x and y values to float**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark = spark.createDataFrame(dfpandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfspark = spark.read.csv(path, header=True) # requires spark 2.0. If the data was saved as a local csv file, we'd read it back this way instead of the cell above.\n",
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting to float can be done in a couple of different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can scan all the \"features\" columns after the first one (which is the id column which is a string) and convert them to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark_feat = dfspark.select(*(dfspark[c].cast(\"float\").alias(c) for c in dfspark.columns[1:]))\n",
    "dfspark_feat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, since we know the names of the feature columns, we can simply target them with the type conversion. We will keep the result of this approach going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dfspark.columns:\n",
    "    if col in FEATURES_COL:\n",
    "        dfspark = dfspark.withColumn(col,dfspark[col].cast('float'))\n",
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop any potential null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark = dfspark.na.drop()\n",
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark implementation of many machine learning algorithms requires that all input columns be \"concatenated\" into a single input vector. This is done with a Vector Assembler transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vectorassembler\"></a>\n",
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\">VectorAssembler</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-1\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-1\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how VectorAssembler works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-1\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.linalg import Vectors <br>\n",
    "from pyspark.ml.feature import VectorAssembler <br>\n",
    "<br>\n",
    "dataset = spark.createDataFrame( <br>\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)], <br>\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"]) <br>\n",
    "<br>\n",
    "assembler = VectorAssembler( <br>\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"], <br>\n",
    "    outputCol=\"features\") <br>\n",
    "<br>\n",
    "output = assembler.transform(dataset) <br>\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\") <br>\n",
    "output.select(\"features\", \"clicked\").show(truncate=False) <br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(dfspark).select('id', 'features')\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 4: Decide the number of clusters 'k'**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the \"downsides\" of the k-means clustering algorithm is that it is not able to choose on its own the number of clusters, which has to be provided as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One way of determining the best number of clusters is to try different values and determine which one yields the lowest cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.zeros(20)\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.1, seed=42))\n",
    "    cost[k] = model.computeCost(df_kmeans) # requires Spark 2.0 or later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the \"elbow\" in the cost curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,20),cost[2:20])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems from the curve above that the ideal value of k is around 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 5: Run k-means with 6 clusters**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(df_kmeans)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(df_kmeans).select('id', 'prediction')\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 6: Join the predictions results with the original dataframe**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = transformed.join(dfspark, 'id')\n",
    "df_pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_pandas = df_pred.toPandas().set_index('id')\n",
    "df_pred_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = plt.figure(figsize=(12,10)).gca(projection='rectilinear')\n",
    "myplot.scatter(df_pred_pandas.x, df_pred_pandas.y, c=df_pred_pandas.prediction)\n",
    "myplot.set_xlabel('x')\n",
    "myplot.set_ylabel('y')\n",
    "#threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = plt.figure(figsize=(12,10)).gca(projection='rectilinear')\n",
    "myplot.scatter(X[:,0], X[:,1], c=y)\n",
    "myplot.set_xlabel('x')\n",
    "myplot.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\"><center>**PART 2: Run k-means with a customers dataset**</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 7: Download the customer data**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below is used when the data file is loaded locally into the Watson Studio object storage. Not used in this scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "#credentials = {\n",
    "#    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "#    'api_key': 'ymzV2WGV_YuRqjW-ysK1LqbbCrwKmhRmCtF8BFPP1aMz',\n",
    "#    'service_id': 'iam-ServiceId-b15106ac-8f38-4585-acbc-dd19c7d847c9',\n",
    "#    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n",
    "\n",
    "#configuration_name = 'os_a36dabc7e3cb4c1b9971724a79e7f4ee_configs'\n",
    "#cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "#customers = spark.read\\\n",
    "#  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "#  .option('header', 'true')\\\n",
    "#  .option('inferschema', 'true')\\\n",
    "#  .load(cos.url('CustomerDataSegmentation.csv', 'm32-donotdelete-pr-6sjzl2grahnlag'))\n",
    "#customers.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run once to install the wget package\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "url_customer='https://raw.githubusercontent.com/DScienceAtScale/K-means/master/data/CustomerDataSegmentation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f CustomerDataSegmentation.csv\n",
    "\n",
    "customerFilename=wget.download(url_customer)\n",
    "\n",
    "#list existing files\n",
    "!ls -l CustomerDataSegmentation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(customerFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 8: Some data preparation**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to put the AGE and INCOME columns into 3 distinct buckets (bucket values will be 0, 1, 2). This is referred to as binning or bucketizing. We can later rerun the clustering with different numbers of buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "customers=customers.withColumn(\"AGE\", customers[\"AGE\"].cast(DoubleType()))\n",
    "customers=customers.withColumn(\"INCOME\", customers[\"INCOME\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = QuantileDiscretizer(numBuckets=3, inputCol=\"AGE\",outputCol=\"AGE_BINS\").fit(customers).transform(customers)\n",
    "customers = QuantileDiscretizer(numBuckets=3, inputCol=\"INCOME\",outputCol=\"INCOME_BINS\").fit(customers).transform(customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define a function to correct some cases where the marital status is undefined. We will (arbitrarily) decide that if the household has more than 2 members then the status is 'Married' otherwise 'Single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertms(MARITAL_STATUS, MEMBERS_IN_HOUSEHOLD):\n",
    "    if  (MARITAL_STATUS == \"S\") or (MARITAL_STATUS == \"M\"): \n",
    "        MARITAL_STATUS\n",
    "    elif (MARITAL_STATUS == \"U\") and (MEMBERS_IN_HOUSEHOLD >= 2):  \n",
    "        MARITAL_STATUS = \"M\"\n",
    "    else:\n",
    "        MARITAL_STATUS = \"S\"\n",
    "\n",
    "    return MARITAL_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "convertmsudf = udf(convertms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataframe customers2 has a derived marital status field, where we attempted to correct undefined marital statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers2 = customers.withColumn(\"MARITAL_STATUS_DERIVED\", convertmsudf(customers[\"MARITAL_STATUS\"], customers['MEMBERS_IN_HOUSEHOLD']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply String Indexers to some string columns which we want to use as input to the clustering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer_ms = StringIndexer(inputCol=\"MARITAL_STATUS_DERIVED\", outputCol=\"MARITAL_STATUS_Indexed\").fit(customers2)\n",
    "customers2 = indexer_ms.transform(customers2) \n",
    "customers2.select(['CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS_DERIVED', 'EDUCATION', 'MARITAL_STATUS_Indexed']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-2\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-2\">\n",
    "        Repeat the string indexer transformation for the GENDER column. Click on this hint to copy / paste the answer if needed.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "indexer_gender = StringIndexer(inputCol=\"GENDER\", outputCol=\"GENDER_Indexed\").fit(customers2) <br>\n",
    "customers2 = indexer_gender.transform(customers2) <br>\n",
    "customers2.select(['CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS_DERIVED', 'EDUCATION', 'GENDER_Indexed']).show()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning, the blank cell above is missing required code. You need to either add the string indexer transformation for the GENDER column as per previous examples, or use the hint in the cell just above the previous one to add the missing code before proceeding with the rest of the notebook</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse1-3\">\n",
    "        Repeat the string indexer transformation for the EDUCATION column. Click on this hint to copy / paste the answer if needed.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "indexer_education = StringIndexer(inputCol=\"EDUCATION\", outputCol=\"EDUCATION_Indexed\").fit(customers2) <br>\n",
    "customers2 = indexer_education.transform(customers2) <br>\n",
    "customers2.select(['CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS_DERIVED', 'EDUCATION', 'EDUCATION_Indexed']).show()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning, the blank cell above is missing required code. You need to either add the string indexer transformation for the EDUCATION column as per previous examples, or use the hint in the cell just above the previous one to add the missing code before proceeding with the rest of the notebook</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-4\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse1-4\">\n",
    "        Repeat the string indexer transformation for the PROFESSION column. Click on this hint to copy / paste the answer if needed.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "indexer_profession = StringIndexer(inputCol=\"PROFESSION\", outputCol=\"PROFESSION_Indexed\").fit(customers2)<br>\n",
    "customers2 = indexer_profession.transform(customers2)<br>\n",
    "customers2.select(['CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS_DERIVED', 'EDUCATION', 'PROFESSION', 'PROFESSION_Indexed']).show()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning, the blank cell above is missing required code. You need to either add the string indexer transformation for the PROFESSION column as per previous examples, or use the hint in the cell just above the previous one to add the missing code before proceeding with the rest of the notebook</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers2=customers2.withColumn(\"agetile\", customers2[\"agetile\"].cast(DoubleType()))\n",
    "#customers2=customers2.withColumn(\"incometile\", customers2[\"incometile\"].cast(DoubleType()))\n",
    "#customers2=customers2.withColumn(\"GENDER_Indexed\", customers2[\"GENDER_Indexed\"].cast(DoubleType()))\n",
    "#customers2=customers2.withColumn(\"MARITAL_STATUS_Index\", customers2[\"MARITAL_STATUS_Index\"].cast(DoubleType()))\n",
    "#customers2=customers2.withColumn(\"EDUCATION_Indexed\", customers2[\"EDUCATION_Indexed\"].cast(DoubleType()))\n",
    "#customers2=customers2.withColumn(\"PROFESSION_Indexed\", customers2[\"PROFESSION_Indexed\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL = ['AGE_BINS','INCOME_BINS','GENDER_Indexed', 'MARITAL_STATUS_Indexed', 'EDUCATION_Indexed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Mini data frame manipulation exercise: Can you check how many rows were switched from a married status of 'U' to either 'M' or 'S' ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-5\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse1-5\">\n",
    "        Check how many rows had the married status of 'U' changed to 'M' or 'S'. Click on this hint to see one possible way to query the answer. You can copy / paste the answer in the empty cell below and execute it</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-5\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "customers2.filter(customers2[\"MARITAL_STATUS\"]!=customers2[\"MARITAL_STATUS_DERIVED\"]).count()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Mini data frame manipulation exercise: How many customers are left with the 'U' married state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-6\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-6\" href=\"#collapse1-6\">\n",
    "        Check how many rows are left with the 'U' married state. Click on this hint to see one possible way to query the answer. You can copy / paste the answer in the empty cell below and execute it</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-6\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "customers2.filter(customers2[\"MARITAL_STATUS_DERIVED\"]=='U').count()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 9: Build the vectorized and call the k-means algorithm**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a df_kmeans dataframe that will have the vector of features ready to be ingested by the Spark k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-7\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-7\" href=\"#collapse1-7\">\n",
    "        Build a vector of features that will be passed into the k-means algorithm. Click on this hint to see a code example that can be used, which you can copy / paste into a blank cell below.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-7\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")<br>\n",
    "df_kmeans = vecAssembler.transform(customers2).select('CUST_ID','features')<br>\n",
    "df_kmeans.show()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning, the blank cell above is missing required code. You need to either add the vector assembler transformation to produce the df_kmeans dataframe as per previous examples, or use the hint in the cell just above the previous one to add the missing code before proceeding with the rest of the notebook</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the optimal number of clusters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning, the couple of cells below need to fill an actual value for the token xxxfill_valuexxx . Based on the generic example covered in the first half of this notebook, proceed with some trials to identify a value for the number of clusters to use in the rest of this lab...</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.zeros(xxxfill_valuexxx)\n",
    "for k in range(2,xxxfill_valuexxx):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.2, seed=42))\n",
    "    cost[k] = model.computeCost(df_kmeans) # requires Spark 2.0 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,xxxfill_valuexxx),cost[2:xxxfill_valuexxx])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Warning, the cell below needs a value for k before being run. Please replace the token xxxfill_valuexxx with an actual number based on findings from running the previous two cells. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = xxxfill_valuexxx\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(df_kmeans)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the df_kmeans dataframe to assign a prediction for each data point (i.e for each customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(df_kmeans).select('CUST_ID', 'prediction')\n",
    "rows = transformed.collect()\n",
    "print(rows[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar to what was previously done, we will now join the predictions with the original rows through the CUST_ID field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = transformed.join(customers2, 'CUST_ID')\n",
    "df_pred.select(['CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS', 'EDUCATION', 'INCOME_BINS', 'prediction']).show()\n",
    "#df_pred.select(['CUST_ID', 'customers2.NAME', 'customers2.AGE', 'customers2.GENDER', 'customers2.MARITAL_STATUS', 'customers2.EDUCATION', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick one cluster and check how many customers it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.filter(df_pred[\"prediction\"]==cluster_number).select('CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS_DERIVED', 'MEMBERS_IN_HOUSEHOLD', 'EDUCATION', 'AGE_BINS', 'INCOME_BINS', 'prediction').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also look at the customers in that same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pred.filter(df_pred[\"prediction\"]==cluster_number).select('CUST_ID', 'NAME', 'AGE', 'GENDER', 'MARITAL_STATUS_DERIVED', 'MEMBERS_IN_HOUSEHOLD', 'EDUCATION', 'AGE_BINS', 'INCOME_BINS', 'prediction').show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise: By varying the cluster_number variable above, can you start analyzing the content of each cluster and the type of customers that it contains? Do the clusters look homogeneous to you? If not, what should be your next steps? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also take a look at the cluster attributes visually..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the predictions dataframe to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_pandas = df_pred.select('prediction', 'AGE_BINS', 'INCOME_BINS', 'GENDER_Indexed', 'MARITAL_STATUS_Indexed', 'EDUCATION_Indexed').filter(df_pred.prediction ==1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use parallel coordinates to visualize the cluster attributes. Note, it is possible to show more than one cluster in the same graph by modifying the 'prediction' predicate in the df_pred_pandas dataframe definition a couple of cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import parallel_coordinates\n",
    "plt.figure(figsize=(12,5))\n",
    "pc = parallel_coordinates(df_pred_pandas, 'prediction', color=('red', 'green', 'blue', 'yellow', 'purple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the values of the various Indexed attributes to recall what they correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"marital status:\",indexer_ms.labels, \"gender:\", indexer_gender.labels, \"education:\", indexer_education.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise: Consider rerunning the clustering with a different number of buckets for the AGE and the INCOME columns? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you have reached the end of this notebook, but this is only the beginning of the process, there is always a lot more to do !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For questions or feedback, please contact:<br>\n",
    "Mokhtar Kandil.<br>\n",
    "mkandil@ca.ibm.com**<br>\n",
    "IBM DTE (Digital Technical Engagements) Big Data and Data Science<br>\n",
    "August 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
